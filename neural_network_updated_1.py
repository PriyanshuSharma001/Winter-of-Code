# -*- coding: utf-8 -*-
"""Neural_Network_Updated-1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M7scJ7XqTz8ov2NPyzjUTphovt5F9IqX
"""

# Importing Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

"""# **Training**"""

# Importing Training Data 
TrainingData = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/emnist-letters-train.csv")
TrainingData

X = np.array(TrainingData.drop('23', axis=1))
print('X Shape : ', X.shape)
print(X)

Y = np.array([TrainingData['23']]).T
print('Y Shape : ', Y.shape)
print(Y)

m = X.shape[0]
print('m =', m)

Classes = np.unique(Y)
print('Classes :\n', Classes)
Y_HotOne = np.zeros((m, len(Classes)))
for i in range(0, m):
  Y_HotOne[i][Y[i]-1] = 1
print('Y_HotOne encoding :\n', Y_HotOne)

X = np.concatenate( (np.ones((m,1)), X), axis=1 )
print('X Shape : ', X.shape)
print(X)

def Sigmoid(X):
    return 1/(1 + np.exp(-X))

LayerSizes = [784, 200, 26]

Weights = {
        'W1' : np.random.rand(LayerSizes[1], LayerSizes[0]+1),
        'W2' : np.random.rand(LayerSizes[2], LayerSizes[1]+1)
    }

print(Weights['W1'])

print(Weights['W2'])

def Cost(Output, Y):
  return -np.sum( (Y*np.log(Output) + (1-Y) * np.log(1-Output))/m )

def ForwardPropogation(X):
    Weights['A1'] = X

    Weights['Z2'] = np.dot(Weights['A1'], Weights["W1"].T)
    Weights['A2'] = Sigmoid(Weights['Z2'])
    Weights['A2'] = np.concatenate((np.ones((X.shape[0],1)), Weights['A2']), axis=1)

    Weights['Z3'] = np.dot(Weights['A2'], Weights["W2"].T)
    Weights['A3'] = Sigmoid(Weights['Z3'])

    return Weights['A3']

CostList = []

def BackwardPropogation(X, Y, LearningRate, Iterations):
    for i in range(Iterations):
      Output = ForwardPropogation(X)
      
      d3 = Weights['A3'] - Y

      d2 = (np.dot(d3,Weights['W2'])) * (Weights['A2'] * (1 - Weights['A2']))
      d2 = d2[:, 1:]

      d_W1 = np.dot(d2.T, Weights['A1'])/m
      Weights['W1'] = Weights['W1'] - LearningRate * d_W1

      d_W2 = np.dot(d3.T, Weights['A2'])/m
      Weights['W2'] = Weights['W2'] - LearningRate * d_W2
      
      CostList.append(Cost(Output, Y))

LearningRate = 0.1
Iterations   = 100

BackwardPropogation(X, Y_HotOne, LearningRate, Iterations)

print(CostList)

# Minimized Cost and Graph
rng = np.arange(0, len(CostList))
plt.plot(rng, CostList)
plt.xlabel('Iterations')
plt.ylabel('Cost')
plt.show()

print(Weights['W1'])

print(Weights['W2'])

count = 0
Prediction = np.array([Weights['A3'].argmax(axis=1)]).T + 1

for i in range(m):
  if(Prediction[i] == Y[i]):
    count+=1

Accuracy = (count/m)*100
print('Accuracy : ', Accuracy, '%')

"""# **Testing**"""

# Importing Testing Data 
TestingData = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/emnist-letters-test.csv")
TestingData

X_test = np.array(TestingData.drop('1', axis=1))
print('X_test Shape : ', X_test.shape)
print(X_test)

Y_test = np.array([TestingData['1']]).T
print('Y_test Shape : ', Y_test.shape)
print(Y_test)

m = X_test.shape[0]
print('m =', m)

X_test = np.concatenate( (np.ones((m,1)), X_test), axis=1 )
print('X_test Shape : ', X_test.shape)
print(X_test)

Output = ForwardPropogation(X_test)

count = 0
Prediction = np.array([Output.argmax(axis=1)]).T + 1

for i in range(m):
  if(Prediction[i] == Y[i]):
    count+=1

Accuracy = (count/m)*100
print('Accuracy : ', Accuracy, '%')

